{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657d3814",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1490137/142771050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import datetime as dt\n",
    "from sqlalchemy import create_engine, text\n",
    "import zipfile\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551033d-401c-4208-a5d3-d77370709268",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0dc43ec-b47c-463a-825c-11123b6ab510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welches Jahr?\n",
    "jahr = \"2021\"\n",
    "# Welcher Zip?\n",
    "zipname = \"20211015_fahrplaene_gesamtdeutschland_gtfs\"\n",
    "# # Welche Routenreferenz? (im raw-directory)\n",
    "routescope = \"\"\n",
    "\n",
    "# define paths\n",
    "workingdir = \"/home/jupyter-maita.schade/VW_Data_Hub/Gap_Map/\"\n",
    "#storagedir = \"smb://192.168.90.30/allmende%20verkehr/4%20Projekte/2%20Projekte%20Mobilitätswende/ÖV-Deutschlandkarte%20(Gap-Map)/Berechnungen/raw/gtfs/\"\n",
    "\n",
    "# constructed paths\n",
    "# rawdir = workingdir + \"raw/\"\n",
    "rawdir = workingdir + \"raw/\"\n",
    "rawdatadir = rawdir + \"gtfs/\" + 'delfi/'# + jahr + \"/\"\n",
    "outdir = workingdir + \"out/\" + 'delfi/'# + jahr + \"/\"\n",
    "#inpath = \"{0}{1}_{2}.db\".format(rawdatadir,jahr,datum)\n",
    "zippath = rawdatadir + zipname + \".zip\"\n",
    "\n",
    "# set up zip file as default for functions\n",
    "zf = zipfile.ZipFile(zippath) # this is the raw stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63176b7-d16c-4414-8a06-c52691d667f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose file-based output connection\n",
    "outpath = \"{0}{1}_test.db\".format(outdir,zipname)\n",
    "# set up DB connection\n",
    "dbout = create_engine('sqlite:///' + outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc197e-2308-469a-b01c-a98f6120b55d",
   "metadata": {},
   "source": [
    "# Count service_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2ac02-c2b0-47cd-8477-ee00ce5b2f9f",
   "metadata": {},
   "source": [
    "# Get things into database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe18de-017a-48ec-b988-c056f9d38a31",
   "metadata": {},
   "source": [
    "## calendar\n",
    "## trips\n",
    "## stop_times\n",
    "## stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d46c1-2510-4083-b97d-3763cf40a34a",
   "metadata": {},
   "source": [
    "# calendar -> trips [service_id, count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba5ada-0fac-471f-9ba0-fa1a75d8e259",
   "metadata": {},
   "source": [
    "# trips -> stop_times [trip_id, count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75743b88-4faf-4580-8597-55c826458791",
   "metadata": {},
   "source": [
    "# Group stop_times [stop_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa916f-98f5-4b8a-b4c1-2990c54f4248",
   "metadata": {},
   "source": [
    "# stop_times -> stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dfe59-5745-4655-b821-49415b56d75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0946fed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Filter for routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89faa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRouteShortNames(scope):\n",
    "    # Relying on pre-separated routes file in raw directory\n",
    "    # takes scope prefix and gets short_names to filter for\n",
    "    # !!! This only works for Fernverkehr!!!\n",
    "    print(\"Scope for routes: \"+ scope)\n",
    "    routespath = [s for s in os.listdir(rawdatadir) if re.search(\"routes_\"+scope, s) ][0]\n",
    "    print(\"Reading good routes from \" + routespath)\n",
    "    scope_routes_df = pd.read_csv(rawdatadir + routespath)\n",
    "    routenames = scope_routes_df.route_short_name.unique()\n",
    "#    routeids = routes_df.route_id.unique()\n",
    "    return(routenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96aadac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope for routes: fv\n",
      "Reading good routes from 20210415_routes_fv.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['EC', 'ECE', 'IC', 'ICE', 'RJ'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRouteShortNames(routescope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f072319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterByRoute(trips_df, scope = routescope, zf = zf):\n",
    "    # Given a list of route_short_names included in scope\n",
    "    # relying on or taking routes and trips in rawdatadir\n",
    "    # takes stop_times and filters them to include only stops made on routes included in scope\n",
    "    if scope != \"\":\n",
    "        routenames = getRouteShortNames(scope)\n",
    "        print(\"Filtering routes...\")\n",
    "        routes_df = pd.read_csv(zf.open(\"routes.txt\"), usecols = [\"route_short_name\", \"route_id\"])\n",
    "\n",
    "        trips_df_filtered = trips_df.merge(\n",
    "            routes_df[routes_df[\"route_short_name\"].isin(routenames)], # which routes are ok?\n",
    "            how=\"right\",\n",
    "            on =\"route_id\"\n",
    "        ) # which trips are on those routes?\n",
    "#         print(\"length now: \", len(trips_df_filtered))\n",
    "    else:\n",
    "        print(\"Not filtering routes...\")\n",
    "        trips_df_filtered = trips_df\n",
    "    print(\"Total trips: \", len(trips_df_filtered))\n",
    "        \n",
    "    return(trips_df_filtered[[\"trip_id\",\"service_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c992492",
   "metadata": {},
   "source": [
    "### Generate counts for `service_id`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6da3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interveningWeekdays(start, end, inclusive=True, weekdays=[0, 1, 2, 3, 4]):\n",
    "    # a useful function from Stackoverflow, to count particular weekdays in date range\n",
    "    if isinstance(start, dt.datetime):\n",
    "        start = start.date()               # make a date from a datetime\n",
    "\n",
    "    if isinstance(end, dt.datetime):\n",
    "        end = end.date()                   # make a date from a datetime\n",
    "\n",
    "    if end < start:\n",
    "        # you can opt to return 0 or swap the dates around instead\n",
    "        # raise ValueError(\"start date must be before end date\")\n",
    "        end, start = start, end\n",
    "\n",
    "    if inclusive:\n",
    "        end += dt.timedelta(days=1)  # correct for inclusivity\n",
    "\n",
    "    try:\n",
    "        # collapse duplicate weekdays\n",
    "        weekdays = {weekday % 7 for weekday in weekdays}\n",
    "    except TypeError:\n",
    "        weekdays = [weekdays % 7]\n",
    "        \n",
    "#     print(weekdays)\n",
    "\n",
    "    ref = dt.date.today()                    # choose a reference date\n",
    "#     print(ref)\n",
    "    ref -= dt.timedelta(days=ref.weekday())  # and normalize its weekday\n",
    "#     print(ref)\n",
    "\n",
    "    # sum up all selected weekdays (max 7 iterations)\n",
    "#     ct = 0\n",
    "#     for weekday in weekdays:\n",
    "#         ref_plus = ref + dt.timedelta(days=weekday)\n",
    "#         inc = (ref_plus - start).days // 7 - (ref_plus - end).days // 7\n",
    "#         ct += inc\n",
    "        \n",
    "#     print(ct)\n",
    "\n",
    "    return sum((ref_plus - start).days // 7 - (ref_plus - end).days // 7\n",
    "               for ref_plus in\n",
    "               (ref + dt.timedelta(days=weekday) for weekday in weekdays))\n",
    "\n",
    "def countDaysInIntervalHelper(calendarrow):\n",
    "    # function to find number of days of service operation based on calendars.txt-entry\n",
    "    servicepattern = calendarrow.loc[\"monday\":\"sunday\"].to_numpy()\n",
    "#     print(servicepattern)\n",
    "    servicedays = servicepattern.nonzero()[0].tolist()\n",
    "#     print(servicedays)\n",
    "    startdate = dt.datetime.strptime(str(int(calendarrow.get(\"start_date\"))),\"%Y%m%d\")\n",
    "    enddate = dt.datetime.strptime(str(int(calendarrow.get(\"end_date\"))),\"%Y%m%d\")\n",
    "#    if enddate < startdate:\n",
    "#        print(\"switched start and end at \", calendarrow.get(\"service_id\"))\n",
    "    return(interveningWeekdays(startdate, enddate, weekdays = servicedays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61724e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add frequencies... let's hope this is right\n",
    "def getServiceCount(zf = zf):\n",
    "    # enriches stop_times DataFrame with information about how often in the feed\n",
    "    # period each stop is made\n",
    "    \n",
    "\n",
    "    print(\"Getting number of service days for each service\")\n",
    "    # use service_id to find service...\n",
    "    \n",
    "\n",
    "    # get regular service from calendar.txt\n",
    "    print(\"\\t...reading regular service calendars\")\n",
    "    calendar_df = pd.read_csv(zf.open(\"calendar.txt\"))\n",
    "    calendar_df[\"days_count\"] = calendar_df.apply(countDaysInIntervalHelper, axis=1)\n",
    "\n",
    "#     calendar_df.to_sql(\"calendar\",db, if_exists = \"replace\")\n",
    "    # and get exceptions from calendar_dates.txt\n",
    "\n",
    "    print(\"\\t...reading calendar exceptions\")\n",
    "    calendar_dates_df = pd.read_csv(zf.open(\"calendar_dates.txt\"))\n",
    "    \n",
    "\n",
    "    print(\"\\t...aggregating calendar\")\n",
    "    calendar_df = calendar_dates_df.groupby([\"service_id\", \"exception_type\"], as_index=False\n",
    "                              ).count(\n",
    "                            ).pivot(index = \"service_id\", columns = \"exception_type\", values = \"date\"\n",
    "                            ).reset_index(\n",
    "                            ).merge(calendar_df, on=\"service_id\", how=\"right\"\n",
    "                            )[['service_id', 1, 2, 'monday',\n",
    "                                  'tuesday',  'wednesday',   'thursday',     'friday',   'saturday',\n",
    "                                  'sunday', 'start_date',   'end_date', 'days_count']]\n",
    "    \n",
    "    print(\"\\t...calculating total in calendar\")\n",
    "    calendar_df.days_count= (calendar_df.days_count + calendar_df[1].fillna(0) - calendar_df[2].fillna(0)\n",
    "                            )\n",
    "    \n",
    "    return(calendar_df[[\"service_id\",\"days_count\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de51cb",
   "metadata": {},
   "source": [
    "### add counts to trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441b24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCountsToTrips(trips_df, service_count_df):\n",
    "    trip_counts_df = trips_df.merge(service_count_df[[\"days_count\",\"service_id\"]], how=\"left\", on=\"service_id\")\n",
    "    return(trip_counts_df[[\"trip_id\",\"days_count\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "339d95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrips(zf = zf):\n",
    "    print(\"\\t...reading trips\")\n",
    "    trips_df = pd.read_csv(zf.open(\"trips.txt\"), usecols = [\"route_id\",\"trip_id\",\"service_id\"])\n",
    "    print(\"\\t...\", len(trips_df))\n",
    "    return(trips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64b068",
   "metadata": {},
   "source": [
    "### add counts to stop_times--chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bd1edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countStopTimes(trip_counts_df, dbout, zf = zf):\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 200000\n",
    "    j = 0\n",
    "    #     index_start = 1\n",
    "\n",
    "    print(\"\\t...merging stop_times with trips\")\n",
    "    for df in pd.read_csv(zf.open(\"stop_times.txt\"), chunksize=chunksize, iterator=True, encoding='utf-8'):\n",
    "\n",
    "        j+=1\n",
    "    #     print(j)\n",
    "        if j%10==0:\n",
    "            print('\\t{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        result_df = df.merge(trip_counts_df, on = \"trip_id\", how=\"left\"\n",
    "                        )[[\"stop_id\", \"days_count\"]]\n",
    "    #     print(\"\\t...\", len(result_df))\n",
    "        if j==1:\n",
    "            result_df.to_sql(\"stop_counts\", dbout, if_exists='replace')\n",
    "        else:\n",
    "            result_df.to_sql(\"stop_counts\", dbout, if_exists='append')\n",
    "    return(dbout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b80c6",
   "metadata": {},
   "source": [
    "### querying grouped counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a092e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def queryGroupCounts(dbout):\n",
    "    print(\"Getting counts grouped by stop...\")\n",
    "    count_df = pd.read_sql_query('SELECT stop_id, SUM(days_count) AS n '\n",
    "                      'FROM stop_counts '\n",
    "                      'GROUP BY stop_id',\n",
    "                      dbout\n",
    "                     )\n",
    "    return(count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441a822",
   "metadata": {},
   "source": [
    "### get and add days in feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86188058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeedDays(zf):\n",
    "    ''' Enriches counted dataframe with average daily count for each stop,\n",
    "    using the feed's calendar information to infer the number of days\n",
    "    '''\n",
    "\n",
    "    \n",
    "    print(\"getting n of days in feed\")\n",
    "    # read necessary aux files\n",
    "    calendar_df = pd.read_csv(zf.open(\"calendar.txt\"))\n",
    "    calendar_dates_df = pd.read_csv(zf.open(\"calendar_dates.txt\"))\n",
    "    \n",
    "    # calculate\n",
    "    startdate =  min(pd.to_datetime(calendar_df.start_date,format=\"%Y%m%d\"))\n",
    "    enddate = max(pd.to_datetime(calendar_df.end_date,format=\"%Y%m%d\"))\n",
    "    excdates = pd.to_datetime(calendar_dates_df.date,format=\"%Y%m%d\")\n",
    "\n",
    "    firstdate = min(startdate, min(excdates))\n",
    "    lastdate = max(enddate, max(excdates))\n",
    "\n",
    "    ndays = (lastdate - firstdate).days\n",
    "    \n",
    "    return(ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef716623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addFeedDays(count_df, total_days):\n",
    "    print(\"Adding average daily count...\")\n",
    "    count_df[\"n_day\"] = count_df.n/total_days\n",
    "    return(count_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05e3c4",
   "metadata": {},
   "source": [
    "### Add stop locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abe28972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addStopLocations(count_df, zf = zf):\n",
    "    print(\"Adding stop locations...\")\n",
    "    stops_df = pd.read_csv(zf.open(\"stops.txt\"))\n",
    "    located_df = stops_df.merge(count_df, how=\"right\", on=\"stop_id\")\n",
    "    return located_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba6781",
   "metadata": {},
   "source": [
    "### String it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e3a721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(routescope, total_days):\n",
    "    # choose scope-based output connection\n",
    "    outpath = \"{0}{1}{2}.db\".format(outdir,zipname,routescope)\n",
    "    # set up DB connection\n",
    "    dbout = create_engine('sqlite:///' + outpath)\n",
    "    csvout = \"{}{}{}.nstops.csv\".format(outdir,zipname,routescope)\n",
    "    \n",
    "    addStopLocations(\n",
    "        addFeedDays(\n",
    "            queryGroupCounts(\n",
    "                countStopTimes(\n",
    "                    addCountsToTrips(\n",
    "                        filterByRoute(\n",
    "                            readTrips(), \n",
    "                            scope = routescope\n",
    "                        ),\n",
    "                        getServiceCount()\n",
    "                    ),\n",
    "                    dbout\n",
    "                )\n",
    "            ),\n",
    "            total_days\n",
    "        )\n",
    "    ).to_csv(csvout)\n",
    "    \n",
    "    print(\"Wrote result to \" + csvout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab32d2",
   "metadata": {},
   "source": [
    "## Do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d0f10b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting n of days in feed\n"
     ]
    }
   ],
   "source": [
    "ndays = getFeedDays(zf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68504eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t...reading trips\n",
      "\t... 1931007\n",
      "Not filtering routes...\n",
      "Total trips:  1931007\n",
      "Getting number of service days for each service\n",
      "\t...reading regular service calendars\n",
      "\t...reading calendar exceptions\n",
      "\t...aggregating calendar\n",
      "\t...calculating total in calendar\n",
      "\t...merging stop_times with trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mschade/anaconda3/envs/gapmapbox/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t17 seconds: completed 2000000 rows\n",
      "\t44 seconds: completed 4000000 rows\n",
      "\t79 seconds: completed 6000000 rows\n",
      "\t120 seconds: completed 8000000 rows\n",
      "\t170 seconds: completed 10000000 rows\n",
      "\t226 seconds: completed 12000000 rows\n",
      "\t291 seconds: completed 14000000 rows\n",
      "\t363 seconds: completed 16000000 rows\n",
      "\t441 seconds: completed 18000000 rows\n",
      "\t527 seconds: completed 20000000 rows\n",
      "\t622 seconds: completed 22000000 rows\n",
      "\t721 seconds: completed 24000000 rows\n",
      "\t829 seconds: completed 26000000 rows\n",
      "\t944 seconds: completed 28000000 rows\n",
      "\t1065 seconds: completed 30000000 rows\n",
      "\t1193 seconds: completed 32000000 rows\n",
      "\t1332 seconds: completed 34000000 rows\n",
      "\t1549 seconds: completed 36000000 rows\n",
      "\t1705 seconds: completed 38000000 rows\n",
      "\t1849 seconds: completed 40000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mschade/anaconda3/envs/gapmapbox/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "process(\"\", ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "process(\"fv\", ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebff17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose scope-based output connection\n",
    "# outpath = \"{0}{1}{2}.db\".format(outdir,zipname,routescope)\n",
    "# # set up DB connection\n",
    "# dbout = create_engine('sqlite:///' + outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c017b6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mschade/anaconda3/envs/gapmapbox/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3357: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# addStopLocations(\n",
    "#     addFeedDays(\n",
    "#         queryGroupCounts(dbout),\n",
    "#         total_days\n",
    "#     )\n",
    "# ).to_csv(\"{}{}{}.nstops.csv\".format(outdir,zipname,routescope))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gapmap",
   "language": "python",
   "name": "gapmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
