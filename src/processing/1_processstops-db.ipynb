{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "657d3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from sqlalchemy import create_engine, text\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551033d-401c-4208-a5d3-d77370709268",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0dc43ec-b47c-463a-825c-11123b6ab510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcher Zip?\n",
    "source = \"delfi\" # where is the feed file? delfi or gtfs.de?\n",
    "zipname = '20220425_fahrplaene_gesamtdeutschland_gtfs' # name of GTFS zipfile\n",
    "\n",
    "# define paths\n",
    "workingdir = \"../../data/\" \n",
    "#storagedir = \"smb://192.168.90.30/allmende%20verkehr/4%20Projekte/2%20Projekte%20Mobilitätswende/ÖV-Deutschlandkarte%20(Gap-Map)/Berechnungen/raw/gtfs/\"\n",
    "\n",
    "# constructed paths\n",
    "rawdir = workingdir + \"raw/\" # where is all the data?\n",
    "gtfsdir = rawdir + source + \"/\" # where zip-file is located\n",
    "outdir = workingdir + \"interim/\" # where do outputfiles go?\n",
    "zippath = gtfsdir + zipname + \".zip\"\n",
    "\n",
    "# set up zip file as default for functions\n",
    "zf = zipfile.ZipFile(zippath) # this is the raw stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c63176b7-d16c-4414-8a06-c52691d667f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose file-based output connection\n",
    "outpath = '{0}{1}.db'.format(outdir,zipname)\n",
    "# set up DB connection\n",
    "dbout = create_engine('sqlite:///' + outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "519bd743-0bea-45fa-93cf-3390d6bc60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file for logging\n",
    "logfile = outdir + zipname + \".log\"\n",
    "with open(logfile, 'a') as f:\n",
    "    f.write('## Eckdaten\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc197e-2308-469a-b01c-a98f6120b55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Count service_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6da3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interveningWeekdays(start, end, inclusive=True, weekdays=[0, 1, 2, 3, 4]):\n",
    "    # a useful function from Stackoverflow, to count particular weekdays in date range\n",
    "    if isinstance(start, dt.datetime):\n",
    "        start = start.date()               # make a date from a datetime\n",
    "\n",
    "    if isinstance(end, dt.datetime):\n",
    "        end = end.date()                   # make a date from a datetime\n",
    "\n",
    "    if end < start:\n",
    "        # you can opt to return 0 or swap the dates around instead\n",
    "        # raise ValueError(\"start date must be before end date\")\n",
    "        end, start = start, end\n",
    "\n",
    "    if inclusive:\n",
    "        end += dt.timedelta(days=1)  # correct for inclusivity\n",
    "\n",
    "    try:\n",
    "        # collapse duplicate weekdays\n",
    "        weekdays = {weekday % 7 for weekday in weekdays}\n",
    "    except TypeError:\n",
    "        weekdays = [weekdays % 7]\n",
    "        \n",
    "    ref = dt.date.today()                    # choose a reference date\n",
    "    ref -= dt.timedelta(days=ref.weekday())  # and normalize its weekday\n",
    "\n",
    "    return sum((ref_plus - start).days // 7 - (ref_plus - end).days // 7\n",
    "               for ref_plus in\n",
    "               (ref + dt.timedelta(days=weekday) for weekday in weekdays))\n",
    "\n",
    "def countDaysInIntervalHelper(calendarrow):\n",
    "    # function to find number of days of service operation based on calendars.txt-entry\n",
    "    servicepattern = calendarrow.loc[\"monday\":\"sunday\"].to_numpy()\n",
    "    servicedays = servicepattern.nonzero()[0].tolist()\n",
    "\n",
    "    startdate = dt.datetime.strptime(str(int(calendarrow.get(\"start_date\"))),\"%Y%m%d\")\n",
    "    enddate = dt.datetime.strptime(str(int(calendarrow.get(\"end_date\"))),\"%Y%m%d\")\n",
    "    return(interveningWeekdays(startdate, enddate, weekdays = servicedays))\n",
    "\n",
    "### Helper function to compare dates\n",
    "def isInIntervalHelper(n, interval):\n",
    "    '''works only on ARRAY-like n'''\n",
    "    return(np.where((n <= max(interval)) & (n >= min(interval)), True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61724e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add frequencies...\n",
    "def addCountToCalendar(calendar_df, calendar_dates_df):\n",
    "    # enriches stop_times DataFrame with information about how often in the feed\n",
    "    # period each stop is made\n",
    "    \n",
    "\n",
    "    print(\"Getting number of service days for each service\")\n",
    "    # use service_id to find service...\n",
    "    calendar_df[\"days_count\"] = calendar_df.apply(countDaysInIntervalHelper, axis=1)    \n",
    "\n",
    "    print(\"\\t...aggregating calendar\")\n",
    "    calendar_df = calendar_dates_df.groupby([\"service_id\", \"exception_type\"], as_index=False\n",
    "                              ).count(\n",
    "                            ).pivot(index = \"service_id\", columns = \"exception_type\", values = \"date\"\n",
    "                            ).reset_index(\n",
    "                            ).merge(calendar_df, on=\"service_id\", how=\"right\"\n",
    "                            )[['service_id', 1, 2, 'monday',\n",
    "                                  'tuesday',  'wednesday',   'thursday',     'friday',   'saturday',\n",
    "                                  'sunday', 'start_date',   'end_date', 'days_count']]\n",
    "    \n",
    "    print(\"\\t...calculating total in calendar\")\n",
    "    calendar_df.days_count= (calendar_df.days_count + calendar_df[1].fillna(0) - calendar_df[2].fillna(0)\n",
    "                            )\n",
    "    \n",
    "    return(calendar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86188058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedDays(calendar_df, calendar_dates_df):\n",
    "    ''' Enriches counted dataframe with average daily count for each stop,\n",
    "    using the feed's calendar information to infer the number of days\n",
    "    '''\n",
    "    # calculate\n",
    "    startdate =  min(pd.to_datetime(calendar_df.start_date,format=\"%Y%m%d\"))\n",
    "    enddate = max(pd.to_datetime(calendar_df.end_date,format=\"%Y%m%d\"))\n",
    "    excdates = pd.to_datetime(calendar_dates_df.date,format=\"%Y%m%d\")\n",
    "\n",
    "    firstdate = min(startdate, min(excdates))\n",
    "    lastdate = max(enddate, max(excdates))\n",
    "\n",
    "    ndays = (lastdate - firstdate).days\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write('First date:\\t{}\\n'.format(firstdate))\n",
    "        f.write('Last date:\\t{}\\n'.format(lastdate))\n",
    "        f.write('Total days:\\t{}\\n\\n'.format(ndays))\n",
    "    print('First date:\\t',firstdate)\n",
    "    print('Last date:\\t',lastdate)\n",
    "    print('Total days:\\t',ndays)\n",
    "    return(ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c3d823a-6ef5-421f-87f5-509dfb1bd5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv(zf.open(\"calendar.txt\"))\n",
    "calendar_dates_df = pd.read_csv(zf.open(\"calendar_dates.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f7b3f40-5ac1-4376-a2bd-42dcd7c4f40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting number of service days for each service\n",
      "\t...aggregating calendar\n",
      "\t...calculating total in calendar\n"
     ]
    }
   ],
   "source": [
    "calendar_df = addCountToCalendar(calendar_df, calendar_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30fbe7f6-ecd2-480d-b0d3-ef11dddd783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date:\t 2022-04-11 00:00:00\n",
      "Last date:\t 2022-12-10 00:00:00\n",
      "Total days:\t 243\n"
     ]
    }
   ],
   "source": [
    "ndays = feedDays(calendar_df, calendar_dates_df) # total number of days in feed period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c7fa1-565c-440b-b0ec-d8dca1366703",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pick out routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a79046-8559-4e6a-8b5d-86bc9854e5d9",
   "metadata": {},
   "source": [
    "This adds a FZ and FB-flag to the routes file, based on previous filtering of FV/NV-routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f3a1f3f-8e2e-4406-958f-23d1d51e7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_routes = pd.read_csv(outdir + zipname + '_fz-routes.csv').route_id\n",
    "fb_routes = pd.read_csv(outdir + zipname + '_fb-routes.csv').route_id\n",
    "\n",
    "routes_df = pd.read_csv(zf.open(\"routes.txt\"))\n",
    "\n",
    "routes_df['fz'] = routes_df.route_id.isin(fz_routes)\n",
    "routes_df['fb'] = routes_df.route_id.isin(fb_routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2ac02-c2b0-47cd-8477-ee00ce5b2f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get things into database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5af5d-b3a6-49bb-a85c-ac8cb16592c9",
   "metadata": {},
   "source": [
    "## calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fce765a-5d77-4a57-85e8-cf3542ed2cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23285"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put enriched calendar into database\n",
    "calendar_df.to_sql(\"calendar\", 'sqlite:///' + outpath,\n",
    "          if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc28c1-a88e-42c9-b824-49bbade58cf2",
   "metadata": {},
   "source": [
    "## routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "696dd25f-15d5-4825-bc00-04455a815d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26191"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routes_df.to_sql(\"routes\", 'sqlite:///' + outpath,\n",
    "          if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551edb3-0099-426d-a4c9-f2d1193761ba",
   "metadata": {},
   "source": [
    "## trips, stops, et al.--everything that goes straight into DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8a08c-1122-409b-95a2-5e2ef6f0bf61",
   "metadata": {},
   "source": [
    "Transfer gtfs-files into database in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8ecf516-1d60-4a08-89b4-c7c3ffa4c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t32 seconds: completed 2000000 rows\n",
      "stop_times\n",
      "\t53 seconds: completed 2000000 rows\n",
      "\t76 seconds: completed 4000000 rows\n",
      "\t98 seconds: completed 6000000 rows\n",
      "\t120 seconds: completed 8000000 rows\n",
      "\t142 seconds: completed 10000000 rows\n",
      "\t164 seconds: completed 12000000 rows\n",
      "\t186 seconds: completed 14000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t208 seconds: completed 16000000 rows\n",
      "\t231 seconds: completed 18000000 rows\n",
      "\t253 seconds: completed 20000000 rows\n",
      "\t275 seconds: completed 22000000 rows\n",
      "\t298 seconds: completed 24000000 rows\n",
      "\t321 seconds: completed 26000000 rows\n",
      "\t343 seconds: completed 28000000 rows\n",
      "\t366 seconds: completed 30000000 rows\n",
      "\t388 seconds: completed 32000000 rows\n",
      "\t412 seconds: completed 34000000 rows\n",
      "\t434 seconds: completed 36000000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:11: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t456 seconds: completed 38000000 rows\n",
      "CPU times: user 6min 58s, sys: 6.56 s, total: 7min 4s\n",
      "Wall time: 7min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = dt.datetime.now()\n",
    "chunksize = 200000\n",
    "\n",
    "ziptables = ['stops','trips', 'stop_times']\n",
    "    \n",
    "    \n",
    "for table_name in ziptables:\n",
    "    print(table_name)\n",
    "\n",
    "    j=0\n",
    "    for df in pd.read_csv(zf.open(table_name + \".txt\"),\n",
    "                          chunksize=chunksize, iterator=True, encoding='utf-8',\n",
    "                           dtype={'Unnamed: 0': 'float64',\n",
    "                           'drop_off_type': 'object',\n",
    "                           'pickup_type': 'object',\n",
    "                           'stop_sequence': 'object',\n",
    "                           'trip_id': 'object',\n",
    "                           'stop_headsign': 'object'}\n",
    "                         ):\n",
    "        j+=1\n",
    "        if j%10==0: # track progress visibly\n",
    "            print('\\t{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        if j==1:\n",
    "            df.to_sql(table_name, dbout, if_exists='replace')\n",
    "        else:\n",
    "            df.to_sql(table_name, dbout, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba5ada-0fac-471f-9ba0-fa1a75d8e259",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff07d9-ce9c-4db5-9773-c95871df3956",
   "metadata": {},
   "source": [
    "### recovery if crashed on db create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9965702-b6b3-4720-b1dd-9794b9c45e4c",
   "metadata": {},
   "source": [
    "* run setup (without logging file)\n",
    "* get feed days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9c1b2-468e-43af-a395-4c151e3bad15",
   "metadata": {},
   "source": [
    "Count stop_times per stop using SQL (to keep large files out of working memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7e363-7f38-447f-b244-85d055b9b1c8",
   "metadata": {},
   "source": [
    "This is where the filtering by route happens--therefore two different queries, one for each route subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d0cf3a-5c59-401f-8cae-950b005f98c1",
   "metadata": {},
   "source": [
    "Zug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65e50291-bc84-4bc0-9096-bc20de6e1a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:02:15.882634\n",
      "CPU times: user 51.6 s, sys: 12.4 s, total: 1min 3s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(dt.datetime.now())\n",
    "\n",
    "fz_out_df = pd.read_sql_query(\n",
    "    'SELECT n_stops.stop_id, n, stop_name, parent_station, stop_lat, stop_lon, location_type '\n",
    "    'FROM ('\n",
    "        'SELECT stop_id, SUM(days_count) AS n '\n",
    "        'FROM ('\n",
    "            'SELECT routes.route_short_name, routes.route_id, trips.service_id, trips.trip_headsign, trips.direction_id, trips.trip_id '\n",
    "            'FROM routes '\n",
    "            'LEFT JOIN trips ON routes.route_id = trips.route_id '\n",
    "            'WHERE routes.fz IS 1 '\n",
    "        ') AS trips_fz '\n",
    "        'LEFT JOIN stop_times ON trips_fz.trip_id = stop_times.trip_id '\n",
    "        'LEFT JOIN calendar ON trips_fz.service_id = calendar.service_id '\n",
    "        'GROUP BY stop_id '\n",
    "    ') AS n_stops '\n",
    "    'JOIN stops ON n_stops.stop_id = stops.stop_id',\n",
    "    dbout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9db4d0-5634-4a39-9aaf-29190f698fea",
   "metadata": {},
   "source": [
    "Bus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2d76bf3-8be1-461b-b6d9-1bc6847ab011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_out_df[\"n_day\"] = fz_out_df.n/ndays # the count per day, for comparing different length feeds\n",
    "fz_out_df.to_csv(outdir + zipname + \".fz.nstops.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd249b82-370b-49f5-8259-1180199b33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:03:28.242176\n",
      "CPU times: user 47.6 s, sys: 7.52 s, total: 55.2 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(dt.datetime.now())\n",
    "\n",
    "fb_out_df = pd.read_sql_query(\n",
    "    'SELECT n_stops.stop_id, n, stop_name, parent_station, stop_lat, stop_lon, location_type '\n",
    "    'FROM ('\n",
    "        'SELECT stop_id, SUM(days_count) AS n '\n",
    "        'FROM ('\n",
    "            'SELECT routes.route_short_name, routes.route_id, trips.service_id, trips.trip_headsign, trips.direction_id, trips.trip_id '\n",
    "            'FROM routes '\n",
    "            'LEFT JOIN trips ON routes.route_id = trips.route_id '\n",
    "            'WHERE routes.fb IS 1 '\n",
    "        ') AS trips_fb '\n",
    "        'LEFT JOIN stop_times ON trips_fb.trip_id = stop_times.trip_id '\n",
    "        'LEFT JOIN calendar ON trips_fb.service_id = calendar.service_id '\n",
    "        'GROUP BY stop_id '\n",
    "    ') AS n_stops '\n",
    "    'JOIN stops ON n_stops.stop_id = stops.stop_id',\n",
    "    dbout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eea62043-ce7d-4208-b38c-582051ad2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_out_df[\"n_day\"] = fb_out_df.n/ndays # the count per day, for comparing different length feeds\n",
    "fb_out_df.to_csv(outdir + zipname + \".fb.nstops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33d131-2d93-414d-85a6-88f4127e1330",
   "metadata": {},
   "source": [
    "Nahverkehr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5deecd6-b029-4351-876f-069d1d8544e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:04:29.779972\n",
      "CPU times: user 1min 32s, sys: 12.1 s, total: 1min 44s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(dt.datetime.now())\n",
    "\n",
    "nv_out_df = pd.read_sql_query(\n",
    "    'SELECT n_stops.stop_id, n, stop_name, parent_station, stop_lat, stop_lon, location_type '\n",
    "    'FROM ('\n",
    "        'SELECT stop_id, SUM(days_count) AS n '\n",
    "        'FROM ('\n",
    "            'SELECT routes.route_short_name, routes.route_id, trips.service_id, trips.trip_headsign, trips.direction_id, trips.trip_id '\n",
    "            'FROM routes '\n",
    "            'LEFT JOIN trips ON routes.route_id = trips.route_id '\n",
    "            'WHERE routes.fz IS 0 AND routes.fb IS 0'\n",
    "        ') AS trips_nv '\n",
    "        'LEFT JOIN stop_times ON trips_nv.trip_id = stop_times.trip_id '\n",
    "        'LEFT JOIN calendar ON trips_nv.service_id = calendar.service_id '\n",
    "        'GROUP BY stop_id '\n",
    "    ') AS n_stops '\n",
    "    'JOIN stops ON n_stops.stop_id = stops.stop_id',\n",
    "    dbout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94e2d10b-6c9f-4a01-bd75-dccfdb0bda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_out_df[\"n_day\"] = nv_out_df.n/ndays # the count per day, for comparing different length feeds\n",
    "nv_out_df.to_csv(outdir + zipname + \".nv.nstops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b054c-f492-4f65-a090-e708caaa3311",
   "metadata": {},
   "source": [
    "# Wrap up and write out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oev-atlas",
   "language": "python",
   "name": "oev-atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
