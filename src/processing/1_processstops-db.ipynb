{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "657d3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import datetime as dt\n",
    "from sqlalchemy import create_engine, text\n",
    "import zipfile\n",
    "# from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551033d-401c-4208-a5d3-d77370709268",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5aead6ad-3949-49f6-b952-a4b386f61a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only for a certain type of route?\n",
    "filter_by_route = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0dc43ec-b47c-463a-825c-11123b6ab510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welches Jahr?\n",
    "jahr = \"2021\"              # year of interest\n",
    "# Welcher Zip?\n",
    "source = \"delfi\" # where is the feed file? delfi or gtfs.de?\n",
    "zipname = '20211015_fahrplaene_gesamtdeutschland_gtfs' # name of GTFS zipfile\n",
    "\n",
    "# define paths\n",
    "workingdir = \"../../data/\" \n",
    "#storagedir = \"smb://192.168.90.30/allmende%20verkehr/4%20Projekte/2%20Projekte%20Mobilitätswende/ÖV-Deutschlandkarte%20(Gap-Map)/Berechnungen/raw/gtfs/\"\n",
    "\n",
    "# constructed paths\n",
    "rawdir = workingdir + \"raw/\" # where is all the data?\n",
    "gtfsdir = rawdir + source + \"/\" # where zip-file is located\n",
    "outdir = workingdir + \"interim/\" # where do outputfiles go?\n",
    "zippath = gtfsdir + zipname + \".zip\"\n",
    "\n",
    "# set up zip file as default for functions\n",
    "zf = zipfile.ZipFile(zippath) # this is the raw stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c63176b7-d16c-4414-8a06-c52691d667f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose file-based output connection\n",
    "outpath = '{0}{1}.db'.format(outdir,zipname)\n",
    "# set up DB connection\n",
    "dbout = create_engine('sqlite:///' + outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc197e-2308-469a-b01c-a98f6120b55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Count service_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6da3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interveningWeekdays(start, end, inclusive=True, weekdays=[0, 1, 2, 3, 4]):\n",
    "    # a useful function from Stackoverflow, to count particular weekdays in date range\n",
    "    if isinstance(start, dt.datetime):\n",
    "        start = start.date()               # make a date from a datetime\n",
    "\n",
    "    if isinstance(end, dt.datetime):\n",
    "        end = end.date()                   # make a date from a datetime\n",
    "\n",
    "    if end < start:\n",
    "        # you can opt to return 0 or swap the dates around instead\n",
    "        # raise ValueError(\"start date must be before end date\")\n",
    "        end, start = start, end\n",
    "\n",
    "    if inclusive:\n",
    "        end += dt.timedelta(days=1)  # correct for inclusivity\n",
    "\n",
    "    try:\n",
    "        # collapse duplicate weekdays\n",
    "        weekdays = {weekday % 7 for weekday in weekdays}\n",
    "    except TypeError:\n",
    "        weekdays = [weekdays % 7]\n",
    "        \n",
    "    ref = dt.date.today()                    # choose a reference date\n",
    "    ref -= dt.timedelta(days=ref.weekday())  # and normalize its weekday\n",
    "\n",
    "    return sum((ref_plus - start).days // 7 - (ref_plus - end).days // 7\n",
    "               for ref_plus in\n",
    "               (ref + dt.timedelta(days=weekday) for weekday in weekdays))\n",
    "\n",
    "def countDaysInIntervalHelper(calendarrow):\n",
    "    # function to find number of days of service operation based on calendars.txt-entry\n",
    "    servicepattern = calendarrow.loc[\"monday\":\"sunday\"].to_numpy()\n",
    "    servicedays = servicepattern.nonzero()[0].tolist()\n",
    "\n",
    "    startdate = dt.datetime.strptime(str(int(calendarrow.get(\"start_date\"))),\"%Y%m%d\")\n",
    "    enddate = dt.datetime.strptime(str(int(calendarrow.get(\"end_date\"))),\"%Y%m%d\")\n",
    "    return(interveningWeekdays(startdate, enddate, weekdays = servicedays))\n",
    "\n",
    "### Helper function to compare dates\n",
    "def isInIntervalHelper(n, interval):\n",
    "    '''works only on ARRAY-like n'''\n",
    "    return(np.where((n <= max(interval)) & (n >= min(interval)), True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61724e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add frequencies...\n",
    "def addCountToCalendar(calendar_df, calendar_dates_df):\n",
    "    # enriches stop_times DataFrame with information about how often in the feed\n",
    "    # period each stop is made\n",
    "    \n",
    "\n",
    "    print(\"Getting number of service days for each service\")\n",
    "    # use service_id to find service...\n",
    "    calendar_df[\"days_count\"] = calendar_df.apply(countDaysInIntervalHelper, axis=1)    \n",
    "\n",
    "    print(\"\\t...aggregating calendar\")\n",
    "    calendar_df = calendar_dates_df.groupby([\"service_id\", \"exception_type\"], as_index=False\n",
    "                              ).count(\n",
    "                            ).pivot(index = \"service_id\", columns = \"exception_type\", values = \"date\"\n",
    "                            ).reset_index(\n",
    "                            ).merge(calendar_df, on=\"service_id\", how=\"right\"\n",
    "                            )[['service_id', 1, 2, 'monday',\n",
    "                                  'tuesday',  'wednesday',   'thursday',     'friday',   'saturday',\n",
    "                                  'sunday', 'start_date',   'end_date', 'days_count']]\n",
    "    \n",
    "    print(\"\\t...calculating total in calendar\")\n",
    "    calendar_df.days_count= (calendar_df.days_count + calendar_df[1].fillna(0) - calendar_df[2].fillna(0)\n",
    "                            )\n",
    "    \n",
    "    return(calendar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86188058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedDays(calendar_df, calendar_dates_df):\n",
    "    ''' Enriches counted dataframe with average daily count for each stop,\n",
    "    using the feed's calendar information to infer the number of days\n",
    "    '''\n",
    "    # calculate\n",
    "    startdate =  min(pd.to_datetime(calendar_df.start_date,format=\"%Y%m%d\"))\n",
    "    enddate = max(pd.to_datetime(calendar_df.end_date,format=\"%Y%m%d\"))\n",
    "    excdates = pd.to_datetime(calendar_dates_df.date,format=\"%Y%m%d\")\n",
    "\n",
    "    firstdate = min(startdate, min(excdates))\n",
    "    lastdate = max(enddate, max(excdates))\n",
    "\n",
    "    ndays = (lastdate - firstdate).days\n",
    "    \n",
    "    return(ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c3d823a-6ef5-421f-87f5-509dfb1bd5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv(zf.open(\"calendar.txt\"))\n",
    "calendar_dates_df = pd.read_csv(zf.open(\"calendar_dates.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7b3f40-5ac1-4376-a2bd-42dcd7c4f40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting number of service days for each service\n",
      "\t...aggregating calendar\n",
      "\t...calculating total in calendar\n"
     ]
    }
   ],
   "source": [
    "calendar_df = addCountToCalendar(calendar_df, calendar_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30fbe7f6-ecd2-480d-b0d3-ef11dddd783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndays = feedDays(calendar_df, calendar_dates_df) # total number of days in feed period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c7fa1-565c-440b-b0ec-d8dca1366703",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pick out routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a79046-8559-4e6a-8b5d-86bc9854e5d9",
   "metadata": {},
   "source": [
    "The below only makes sense if you are filtering a feed by routes. We did this with the gtfs.de dataset, using a partial FV-Export to determine which routes to include.\n",
    "\n",
    "In that case, make sure the correct zip file with the routes you want to include is listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854bd164-1ade-445d-86aa-c255f02b9091",
   "metadata": {},
   "source": [
    "!! Set filter_by_route-flag up top to determine whether this happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f3a1f3f-8e2e-4406-958f-23d1d51e7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_route:\n",
    "    fv_routes = pd.read_csv(zipfile.ZipFile(gtfsdir + \"fv_211028.zip\"\n",
    "                        ).open(\"routes.txt\"))\n",
    "\n",
    "    routes_df = pd.read_csv(zf.open(\"routes.txt\"))\n",
    "\n",
    "    routes_df = fv_routes[[\"route_long_name\"]\n",
    "                    ].merge(routes_df, how=\"inner\",on=\"route_long_name\"\n",
    "                    ).sort_values(\"route_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2ac02-c2b0-47cd-8477-ee00ce5b2f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get things into database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5af5d-b3a6-49bb-a85c-ac8cb16592c9",
   "metadata": {},
   "source": [
    "## calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fce765a-5d77-4a57-85e8-cf3542ed2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put enriched calendar into database\n",
    "calendar_df.to_sql(\"calendar\", 'sqlite:///' + outpath,\n",
    "          if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc28c1-a88e-42c9-b824-49bbade58cf2",
   "metadata": {},
   "source": [
    "## routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "696dd25f-15d5-4825-bc00-04455a815d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if filtering by routes, put routes file into database\n",
    "if filter_by_route == True:\n",
    "    routes_df.to_sql(\"routes\", 'sqlite:///' + outpath,\n",
    "              if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551edb3-0099-426d-a4c9-f2d1193761ba",
   "metadata": {},
   "source": [
    "## trips, stops, et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8a08c-1122-409b-95a2-5e2ef6f0bf61",
   "metadata": {},
   "source": [
    "Transfer gtfs-files into database in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8ecf516-1d60-4a08-89b4-c7c3ffa4c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stops\n",
      "trips\n",
      "stop_times\n",
      "\t139 seconds: completed 2000000 rows\n",
      "\t177 seconds: completed 4000000 rows\n",
      "\t214 seconds: completed 6000000 rows\n",
      "\t250 seconds: completed 8000000 rows\n",
      "\t286 seconds: completed 10000000 rows\n",
      "\t323 seconds: completed 12000000 rows\n",
      "\t361 seconds: completed 14000000 rows\n",
      "\t398 seconds: completed 16000000 rows\n",
      "\t436 seconds: completed 18000000 rows\n",
      "\t475 seconds: completed 20000000 rows\n",
      "\t512 seconds: completed 22000000 rows\n",
      "\t551 seconds: completed 24000000 rows\n",
      "\t592 seconds: completed 26000000 rows\n",
      "\t628 seconds: completed 28000000 rows\n",
      "\t668 seconds: completed 30000000 rows\n",
      "\t706 seconds: completed 32000000 rows\n",
      "routes\n",
      "CPU times: user 9min 55s, sys: 11.3 s, total: 10min 7s\n",
      "Wall time: 12min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = dt.datetime.now()\n",
    "chunksize = 200000\n",
    "\n",
    "if filter_by_route == True: # if we already wrote filtered routes\n",
    "    ziptables = ['stops','trips', 'stop_times']\n",
    "else: # otherwise, do it together with all the other tables\n",
    "    ziptables = ['stops','trips', 'stop_times', 'routes']\n",
    "    \n",
    "    \n",
    "for table_name in ziptables:\n",
    "    print(table_name)\n",
    "\n",
    "    j=0\n",
    "    for df in pd.read_csv(zf.open(table_name + \".txt\"),\n",
    "                          chunksize=chunksize, iterator=True, encoding='utf-8',\n",
    "                           dtype={'Unnamed: 0': 'float64',\n",
    "                           'drop_off_type': 'object',\n",
    "                           'pickup_type': 'object',\n",
    "                           'stop_sequence': 'object',\n",
    "                           'trip_id': 'object',\n",
    "                           'stop_headsign': 'object'}\n",
    "                         ):\n",
    "        j+=1\n",
    "        if j%10==0: # track progress visibly\n",
    "            print('\\t{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))\n",
    "\n",
    "        if j==1:\n",
    "            df.to_sql(table_name, dbout, if_exists='replace')\n",
    "        else:\n",
    "            df.to_sql(table_name, dbout, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba5ada-0fac-471f-9ba0-fa1a75d8e259",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9c1b2-468e-43af-a395-4c151e3bad15",
   "metadata": {},
   "source": [
    "Count stop_times per stop using SQL (to keep large files out of working memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639475f7-d3bb-4b47-b5e9-d717e4abc2f3",
   "metadata": {},
   "source": [
    "    CREATE TABLE n_stops AS\n",
    "       ...> SELECT stop_id, SUM(days_count)\n",
    "       ...> FROM stop_times\n",
    "       ...> LEFT JOIN trips ON trips.trip_id = stop_times.trip_id\n",
    "       ...> LEFT JOIN calendar ON trips.service_id = calendar.service_id\n",
    "       ...> GROUP BY stop_id;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7e363-7f38-447f-b244-85d055b9b1c8",
   "metadata": {},
   "source": [
    "This is where the filtering by route happens--therefore two different queries, one for filtering by route and one for including all stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "650a5905-e425-436f-be27-7bf00195e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-07 14:29:43.027930\n",
      "CPU times: user 1min 10s, sys: 12.6 s, total: 1min 23s\n",
      "Wall time: 11min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(dt.datetime.now())\n",
    "\n",
    "if filter_by_route:\n",
    "    out_df = pd.read_sql_query(\n",
    "        'SELECT n_stops.stop_id, n, stop_name, parent_station, stop_lat, stop_lon, location_type '\n",
    "        'FROM ('\n",
    "            'SELECT stop_id, SUM(days_count) AS n '\n",
    "            'FROM ('\n",
    "                'SELECT routes.route_short_name, routes.route_id, trips.service_id, trips.trip_headsign, trips.direction_id, trips.trip_id '\n",
    "                'FROM routes '\n",
    "                'LEFT JOIN trips ON routes.route_id = trips.route_id '\n",
    "            ') AS trips_fv '\n",
    "            'LEFT JOIN stop_times ON trips_fv.trip_id = stop_times.trip_id '\n",
    "            'LEFT JOIN calendar ON trips_fv.service_id = calendar.service_id '\n",
    "            'GROUP BY stop_id '\n",
    "        ') AS n_stops '\n",
    "        'JOIN stops ON n_stops.stop_id = stops.stop_id',\n",
    "        dbout\n",
    "    )\n",
    "\n",
    "else:\n",
    "    out_df = pd.read_sql_query(\n",
    "        'SELECT n_stops.stop_id, n, stop_name, parent_station, stop_lat, stop_lon, location_type '\n",
    "        'FROM ('\n",
    "            'SELECT stop_id, SUM(days_count) AS n '\n",
    "            'FROM trips '\n",
    "            'LEFT JOIN stop_times ON trips.trip_id = stop_times.trip_id '\n",
    "            'LEFT JOIN calendar ON trips.service_id = calendar.service_id '\n",
    "            'GROUP BY stop_id '\n",
    "        ') AS n_stops '\n",
    "        'JOIN stops ON n_stops.stop_id = stops.stop_id',\n",
    "        dbout\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b054c-f492-4f65-a090-e708caaa3311",
   "metadata": {},
   "source": [
    "# Wrap up and write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b098aa70-2b2e-48e4-a8ee-e05eaef36da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df[\"n_day\"] = out_df.n/ndays # the count per day, for comparing different length feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6918e5a6-bd94-4a7d-84be-8c52dedc12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_by_route:\n",
    "    out_df.to_csv(outdir + zipname + \".fv.nstops.csv\")\n",
    "else:\n",
    "    out_df.to_csv(outdir + zipname + \".nstops.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
